{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Discrete Data 1 - Cancer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "\n",
    "from math import sqrt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data\n",
    "data = pd.read_csv('dataDiscrete/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 5 entries of dataframe\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Benign</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Malignant</td>\n",
       "      <td>0.372583</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           diagnosis  Percent\n",
       "Benign      0.627417       63\n",
       "Malignant   0.372583       37"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data preprocessing\n",
    "data_count=data.diagnosis.value_counts(normalize = True)\n",
    "data_count = pd.Series(data_count)\n",
    "data_count = pd.DataFrame(data_count)\n",
    "data_count.index = ['Benign', 'Malignant']\n",
    "\n",
    "data_count['Percent'] = 100*data_count['diagnosis']/sum(data_count['diagnosis'])\n",
    "data_count['Percent'] = data_count['Percent'].round().astype('int')\n",
    "data_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# changing categorical data to numerical \n",
    "data['diagnosis']= data['diagnosis'].map({'M':1,'B':0})\n",
    "\n",
    "\n",
    "# checking the different values contained in the diagnosis column\n",
    "#Benign : 0\n",
    "#Malign : 1\n",
    "\n",
    "data['diagnosis'].value_counts()\n",
    "\n",
    "# it shows that all the data are unique that is.\n",
    "data['id'].nunique()\n",
    "\n",
    "# here data in last column is empty and id is unique, so removing this does not affect data\n",
    "\n",
    "data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Data dimensions before reduction: (569, 31)\n",
      " Data dimensions after reduction: (569, 2)\n"
     ]
    }
   ],
   "source": [
    "### Applying Dimensionality Reduction\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # Import\n",
    "scaler = StandardScaler() # Instantiate\n",
    "scaler.fit(data) # Fit\n",
    "scaled_data = scaler.transform(data) # Transfor\n",
    "\n",
    "\n",
    "# Applying PCA\n",
    "\n",
    "from sklearn.decomposition import PCA  # Import\n",
    "\n",
    "pca = PCA(n_components=2)  # Instantiate\n",
    "\n",
    "pca.fit(scaled_data)  # Fit\n",
    "\n",
    "X_pca = pca.transform(scaled_data)  # Transform\n",
    "\n",
    "print(\" Data dimensions before reduction:\",scaled_data.shape) \n",
    "print(\" Data dimensions after reduction:\",X_pca.shape) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data set size :  (398, 30)\n",
      "Test data set size :  (171, 30)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the data set into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features = data.drop(columns = ['diagnosis'])\n",
    "target = data['diagnosis']\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(features, target, test_size = 0.3,random_state = 0)\n",
    "\n",
    "print (\"Train data set size : \", X_train1.shape)\n",
    "print (\"Test data set size : \", X_test1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer dataset \n",
      "\n",
      "\n",
      "     Mean    Standard Deviation\n",
      "KNN: 0.919679 (0.018417)\n",
      "confusion matrix\n",
      "[[106   2]\n",
      " [  4  59]]\n",
      "TP - True Negative 106\n",
      "FP - False Positive 2\n",
      "FN - False Negative 4\n",
      "TP - True Positive 59\n",
      "Accuracy Rate: 0.9649122807017544\n",
      "True error (with 80% Confidence Interval):  0.011824096788044459\n",
      "True error (with 90% Confidence Interval):  0.015172105472958762\n",
      "Pessimistic error: 0.03508771929824561\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     Mean    Standard Deviation\n",
      "ID3: 0.916923 (0.032763)\n",
      "confusion matrix\n",
      "[[108   0]\n",
      " [  0  63]]\n",
      "TP - True Negative 108\n",
      "FP - False Positive 0\n",
      "FN - False Negative 0\n",
      "TP - True Positive 63\n",
      "Accuracy Rate: 1.0\n",
      "True error (with 80% Confidence Interval):  0.0\n",
      "True error (with 90% Confidence Interval):  0.0\n",
      "Pessimistic error: 0.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "     Mean    Standard Deviation\n",
      "NB: 0.942244 (0.014880)\n",
      "confusion matrix\n",
      "[[104   4]\n",
      " [  6  57]]\n",
      "TP - True Negative 104\n",
      "FP - False Positive 4\n",
      "FN - False Negative 6\n",
      "TP - True Positive 57\n",
      "Accuracy Rate: 0.9415204678362573\n",
      "True error (with 80% Confidence Interval):  0.015078679724556699\n",
      "True error (with 90% Confidence Interval):  0.019348227883694048\n",
      "Pessimistic error: 0.05847953216374269\n",
      "\n",
      "\n",
      "-> 5-Fold cross-validation accurcay score for the training data for above classifiers\n",
      "test results [0.9649122807017544, 1.0, 0.9415204678362573]\n"
     ]
    }
   ],
   "source": [
    "# applying 5NN , id3, Naive bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import  StratifiedKFold\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from math import sqrt\n",
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "models.append(( 'KNN' , KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))\n",
    "models.append(( 'ID3' , DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\n",
    "models.append(( 'NB' , GaussianNB()))\n",
    "\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 5\n",
    "num_instances = len(X_train1)\n",
    "seed = 7 \n",
    "scoring =  'accuracy'\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 5\n",
    "num_instances = len(X_train1)\n",
    "seed = 7 \n",
    "scoring =  'accuracy'\n",
    "results = []\n",
    "names = []\n",
    "print(\"Cancer dataset \")\n",
    "\n",
    "for name, model in models:\n",
    " #applying straified 5 fold technique\n",
    " cv_results = StratifiedKFold(n_splits=5, random_state=1)\n",
    "   # calculating scores \n",
    " scores = cross_val_score(model, X_train1, y_train1, scoring='accuracy', cv=cv_results, n_jobs=-1)\n",
    " model.fit(X_test1, y_test1) \n",
    " pred = model.predict(X_test1)\n",
    " \n",
    " results.append(model.score(X_test1, y_test1))\n",
    " names.append(name)\n",
    " msg = \"%s: %f (%f)\" % (name,scores.mean(), scores.std())\n",
    " print('\\n')\n",
    "#mean and standard deviation of prediction\n",
    " print('     Mean    Standard Deviation')\n",
    " print(msg)\n",
    "#printing values for comparision ;ike confusion matrix and accuracy \n",
    " print('confusion matrix')\n",
    " print(confusion_matrix(y_test1,pred))\n",
    " \n",
    " # Print out confusion matrix\n",
    " cmat = confusion_matrix(y_test1, pred)\n",
    "#print(cmat)\n",
    " print('TP - True Negative {}'.format(cmat[0,0]))\n",
    " print('FP - False Positive {}'.format(cmat[0,1]))\n",
    " print('FN - False Negative {}'.format(cmat[1,0]))\n",
    " print('TP - True Positive {}'.format(cmat[1,1]))\n",
    " Accuracy = (np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat)))\n",
    " print('Accuracy Rate:',Accuracy)\n",
    " #interval = z * sqrt( (accuracy * (1 - accuracy)) / n)\n",
    "    #confidence interval with 80% \n",
    " confidence_interval = 1.282 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    " print(\"True error (with 80% Confidence Interval): \", confidence_interval)\n",
    " confidence_interval2 = 1.645 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    " print(\"True error (with 90% Confidence Interval): \", confidence_interval2)\n",
    "    # printing pessimistic error of the   \n",
    " print('Pessimistic error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))\n",
    " print('\\n')\n",
    "     \n",
    "print('-> 5-Fold cross-validation accurcay score for the training data for above classifiers')\n",
    "print('test results',results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RIPPER(n_discretize_bins=10, verbosity=0, max_total_conds=None, max_rules=None, random_state=None, dl_allowance=64, k=2, max_rule_conds=None, prune_size=0.33)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# applying ripper algorithm\n",
    "import wittgenstein as lw\n",
    "ripper_clf = lw.RIPPER() # Or irep_clf = lw.IREP() to build a model using IREP\n",
    "ripper_clf.fit( X_train1, y_train1) # Or pass X and y data to .fit\n",
    "ripper_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Ruleset [perimeter_worst=140.9-168.2] V [perimeter_worst=119.4-140.9] V [perimeter_worst=168.2-251.2] V [perimeter_worst=105.9-119.4^perimeter_mean=86.91-92.41] V [symmetry_worst=0.37-0.66] V [texture_mean=21.38-22.55^concavepoints_mean=0.07-0.09] V [symmetry_se=0.01-0.01^texture_mean=21.38-22.55] V [perimeter_worst=105.9-119.4^concavity_se=0.05-0.06] V [area_se=40.73-63.33^symmetry_mean=0.16-0.16]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ruelset of ripper algo\n",
    "ripper_clf.ruleset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[104   4]\n",
      " [  7  56]]\n",
      "\n",
      "\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       108\n",
      "           1       0.93      0.89      0.91        63\n",
      "\n",
      "    accuracy                           0.94       171\n",
      "   macro avg       0.94      0.93      0.93       171\n",
      "weighted avg       0.94      0.94      0.94       171\n",
      "\n",
      "TP - True Negative 104\n",
      "FP - False Positive 4\n",
      "FN - False Negative 7\n",
      "TP - True Positive 56\n",
      "Accuracy Rate: 0.935672514619883\n",
      "True error (with 80% Confidence Interval):  0.015765462384059285\n",
      "True error (with 90% Confidence Interval):  0.020229473963945026\n",
      "Pessimistic Error: 0.06432748538011696\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = ripper_clf.predict(X_test1)\n",
    "\n",
    "print(\"Confusion Matrix\")\n",
    "print(confusion_matrix(y_test1,pred))\n",
    "print('\\n')\n",
    "print(\"classification report\")\n",
    "print(classification_report(y_test1,pred))\n",
    "# Print out confusion matrix\n",
    "cmat = confusion_matrix(y_test1, pred)\n",
    "#print(cmat)\n",
    "print('TP - True Negative {}'.format(cmat[0,0]))\n",
    "print('FP - False Positive {}'.format(cmat[0,1]))\n",
    "print('FN - False Negative {}'.format(cmat[1,0]))\n",
    "print('TP - True Positive {}'.format(cmat[1,1]))\n",
    "Accuracy = (np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat)))\n",
    "print('Accuracy Rate:',Accuracy)\n",
    " #interval = z * sqrt( (accuracy * (1 - accuracy)) / n)\n",
    "    #confidence interval with 80% \n",
    "confidence_interval = 1.282 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    "print(\"True error (with 80% Confidence Interval): \", confidence_interval)\n",
    "confidence_interval2 = 1.645 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    "print(\"True error (with 90% Confidence Interval): \", confidence_interval2)\n",
    "print('Pessimistic Error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Continuous Data - Census/Adult Income dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "from time import time\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import kurtosistest\n",
    "\n",
    "# displaying for notebooks\n",
    "%matplotlib inline\n",
    "                   \n",
    "columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation', \n",
    "           'relationship', 'race','sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "# Load the Census dataset\n",
    "census = pd.read_csv('dataConti/adult.data', header=None, names=columns, skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Success - Display the first 5 record\n",
    "display(census.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>12</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>7688</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&gt;50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>?</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>?</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  workclass     education  education-num      marital-status  \\\n",
       "0   25    Private          11th              7       Never-married   \n",
       "1   38    Private       HS-grad              9  Married-civ-spouse   \n",
       "2   28  Local-gov    Assoc-acdm             12  Married-civ-spouse   \n",
       "3   44    Private  Some-college             10  Married-civ-spouse   \n",
       "4   18          ?  Some-college             10       Never-married   \n",
       "\n",
       "          occupation relationship   race     sex  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
       "1    Farming-fishing      Husband  White    Male             0             0   \n",
       "2    Protective-serv      Husband  White    Male             0             0   \n",
       "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
       "4                  ?    Own-child  White  Female             0             0   \n",
       "\n",
       "   hours-per-week native-country income  \n",
       "0              40  United-States  <=50K  \n",
       "1              50  United-States  <=50K  \n",
       "2              40  United-States   >50K  \n",
       "3              40  United-States   >50K  \n",
       "4              30  United-States  <=50K  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(16281, 14)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Drop the fnlwgt column which is useless for later analysis\n",
    "census = census.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Read in test data\n",
    "census_test = pd.read_csv('dataConti/adult.test', header=None, skiprows=1, names=columns, skipinitialspace=True)\n",
    "\n",
    "# Drop the fnlwgt column which is useless for later analysis\n",
    "census_test = census_test.drop('fnlwgt', axis=1)\n",
    "\n",
    "# Remove '.' in income column\n",
    "census_test['income'] = census_test['income'].apply(lambda x: '>50K' if x=='>50K.' else '<=50K')\n",
    "\n",
    "# Review several rows and shape of data set\n",
    "display(census_test.head())\n",
    "display(census_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occupation        0.056601\n",
       "workclass         0.056386\n",
       "native-country    0.017905\n",
       "income            0.000000\n",
       "hours-per-week    0.000000\n",
       "capital-loss      0.000000\n",
       "capital-gain      0.000000\n",
       "sex               0.000000\n",
       "race              0.000000\n",
       "relationship      0.000000\n",
       "marital-status    0.000000\n",
       "education-num     0.000000\n",
       "education         0.000000\n",
       "age               0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert '?' to NaNs and remove the entries with NaN value\n",
    "object_col = census.select_dtypes(include=object).columns.tolist()\n",
    "for col in object_col:\n",
    "    census.loc[census[col]=='?', col] = np.nan\n",
    "    census_test.loc[census_test[col]=='?', col] = np.nan\n",
    "\n",
    "# Perform an mssing assessment in each column of the dataset.\n",
    "col_missing_pct = census.isna().sum()/census.shape[0]\n",
    "col_missing_pct.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing the missing value:\n",
      "Training set has 30162 samples.\n",
      "Testing set has 15060 samples.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Removing data entries with missing value\n",
    "adult_train = census.dropna(axis=0, how='any')\n",
    "adult_test = census_test.dropna(axis=0, how='any')\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"After removing the missing value:\")\n",
    "print(\"Training set has {} samples.\".format(adult_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(adult_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.301370</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.667492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.452055</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.122449</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.493151</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>Private</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>Cuba</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age         workclass  education  education-num      marital-status  \\\n",
       "0  0.301370         State-gov  Bachelors       0.800000       Never-married   \n",
       "1  0.452055  Self-emp-not-inc  Bachelors       0.800000  Married-civ-spouse   \n",
       "2  0.287671           Private    HS-grad       0.533333            Divorced   \n",
       "3  0.493151           Private       11th       0.400000  Married-civ-spouse   \n",
       "4  0.150685           Private  Bachelors       0.800000  Married-civ-spouse   \n",
       "\n",
       "          occupation   relationship   race     sex  capital-gain  \\\n",
       "0       Adm-clerical  Not-in-family  White    Male      0.667492   \n",
       "1    Exec-managerial        Husband  White    Male      0.000000   \n",
       "2  Handlers-cleaners  Not-in-family  White    Male      0.000000   \n",
       "3  Handlers-cleaners        Husband  Black    Male      0.000000   \n",
       "4     Prof-specialty           Wife  Black  Female      0.000000   \n",
       "\n",
       "   capital-loss  hours-per-week native-country  \n",
       "0           0.0        0.397959  United-States  \n",
       "1           0.0        0.122449  United-States  \n",
       "2           0.0        0.397959  United-States  \n",
       "3           0.0        0.397959  United-States  \n",
       "4           0.0        0.397959           Cuba  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>Private</td>\n",
       "      <td>11th</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.287671</td>\n",
       "      <td>Private</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Farming-fishing</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.150685</td>\n",
       "      <td>Local-gov</td>\n",
       "      <td>Assoc-acdm</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Protective-serv</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.369863</td>\n",
       "      <td>Private</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.777174</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.397959</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>Private</td>\n",
       "      <td>10th</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Other-service</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295918</td>\n",
       "      <td>United-States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  workclass     education  education-num      marital-status  \\\n",
       "0  0.109589    Private          11th       0.400000       Never-married   \n",
       "1  0.287671    Private       HS-grad       0.533333  Married-civ-spouse   \n",
       "2  0.150685  Local-gov    Assoc-acdm       0.733333  Married-civ-spouse   \n",
       "3  0.369863    Private  Some-college       0.600000  Married-civ-spouse   \n",
       "5  0.232877    Private          10th       0.333333       Never-married   \n",
       "\n",
       "          occupation   relationship   race   sex  capital-gain  capital-loss  \\\n",
       "0  Machine-op-inspct      Own-child  Black  Male      0.000000           0.0   \n",
       "1    Farming-fishing        Husband  White  Male      0.000000           0.0   \n",
       "2    Protective-serv        Husband  White  Male      0.000000           0.0   \n",
       "3  Machine-op-inspct        Husband  Black  Male      0.777174           0.0   \n",
       "5      Other-service  Not-in-family  White  Male      0.000000           0.0   \n",
       "\n",
       "   hours-per-week native-country  \n",
       "0        0.397959  United-States  \n",
       "1        0.500000  United-States  \n",
       "2        0.397959  United-States  \n",
       "3        0.397959  United-States  \n",
       "5        0.295918  United-States  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_col = adult_train.dtypes[adult_train.dtypes != 'object'].index\n",
    "\n",
    "# Split the data into features and target label\n",
    "income_raw = adult_train['income']\n",
    "feature_raw = adult_train.drop('income', axis=1)\n",
    "\n",
    "income_raw_test = adult_test['income']\n",
    "feature_raw_test = adult_test.drop('income', axis=1)\n",
    "\n",
    "# Log transform the skewed feature highly-skewed feature 'capital-gain' and 'capital-loss'. \n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "census_log = pd.DataFrame(data=feature_raw)\n",
    "census_log[skewed] = feature_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "census_log_test = pd.DataFrame(data=feature_raw_test)\n",
    "census_log_test[skewed] = feature_raw_test[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "\n",
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = census_log)\n",
    "features_log_minmax_transform[num_col] = scaler.fit_transform(census_log[num_col])\n",
    "\n",
    "# Transform the test data set\n",
    "features_log_minmax_transform_test = pd.DataFrame(data = census_log_test)\n",
    "features_log_minmax_transform_test[num_col] = scaler.transform(census_log_test[num_col])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head())\n",
    "display(features_log_minmax_transform_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_raw\n",
    "\n",
    "y =income_raw\n",
    "\n",
    "# splitting data to train and test for further preprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "categorical = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'sex', 'native-country']\n",
    "for feature in categorical:\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        X_train[feature] = le.fit_transform(X_train[feature])\n",
    "        X_test[feature] = le.transform(X_test[feature])\n",
    "\n",
    "# Changing the income column into Numerical Value\n",
    "y_train = y_train.map({'<=50K':0, '>50K':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Census dataset \n",
      "     Mean    Standard Deviation\n",
      "KNN: 0.818216 (0.004867)\n",
      "confusion matrix\n",
      "[[6307  457]\n",
      " [ 792 1493]]\n",
      "TP - True Negative 6307\n",
      "FP - False Positive 457\n",
      "FN - False Negative 792\n",
      "TP - True Positive 1493\n",
      "Accuracy Rate: 0.8619736987512432\n",
      "True error (with 80% Confidence Interval):  0.0030432734954399443\n",
      "True error (with 90% Confidence Interval):  0.003904980421215841\n",
      "Pessimistic Error: 0.13802630124875676\n",
      "\n",
      "\n",
      "     Mean    Standard Deviation\n",
      "ID3: 0.810969 (0.009602)\n",
      "confusion matrix\n",
      "[[6756    8]\n",
      " [ 103 2182]]\n",
      "TP - True Negative 6756\n",
      "FP - False Positive 8\n",
      "FN - False Negative 103\n",
      "TP - True Positive 2182\n",
      "Accuracy Rate: 0.9877334512100785\n",
      "True error (with 80% Confidence Interval):  0.0009711674716237582\n",
      "True error (with 90% Confidence Interval):  0.0012461548290336054\n",
      "Pessimistic Error: 0.012266548789921538\n",
      "\n",
      "\n",
      "     Mean    Standard Deviation\n",
      "NB: 0.804481 (0.006848)\n",
      "confusion matrix\n",
      "[[5886  878]\n",
      " [ 935 1350]]\n",
      "TP - True Negative 5886\n",
      "FP - False Positive 878\n",
      "FN - False Negative 935\n",
      "TP - True Positive 1350\n",
      "Accuracy Rate: 0.7996463697646149\n",
      "True error (with 80% Confidence Interval):  0.003531511658043248\n",
      "True error (with 90% Confidence Interval):  0.0045314638669899715\n",
      "Pessimistic Error: 0.20035363023538513\n",
      "\n",
      "\n",
      "-> 5-Fold cross-validation accurcay score for the training data for above classifiers\n",
      "test results [0.8619736987512432, 0.9877334512100785, 0.7996463697646149]\n"
     ]
    }
   ],
   "source": [
    "# applying 5NN , id3, Naive bayes\n",
    " \n",
    "    \n",
    "\n",
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "models.append(( 'KNN' , KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))\n",
    "models.append(( 'ID3' , DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\n",
    "models.append(( 'NB' , GaussianNB()))\n",
    "\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 5\n",
    "num_instances = len(X_train.dtypes[X_train.dtypes != 'object'].index)\n",
    "seed = 7 \n",
    "scoring =  'accuracy'\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 5\n",
    "num_instances = len(X_train)\n",
    "seed = 7 \n",
    "scoring =  'accuracy'\n",
    "results = []\n",
    "names = []\n",
    "print(\"For Census dataset \")\n",
    "\n",
    "for name, model in models:\n",
    " \n",
    " cv_results = StratifiedKFold(n_splits=5, random_state=1)\n",
    " scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv_results, n_jobs=-1)\n",
    " \n",
    " model.fit(X_test, y_test) \n",
    " pred = model.predict(X_test)\n",
    " \n",
    " results.append(model.score(X_test, y_test))\n",
    " names.append(name)\n",
    " msg = \"%s: %f (%f)\" % (name,scores.mean(), scores.std())\n",
    " print('     Mean    Standard Deviation')\n",
    " print(msg)\n",
    " print('confusion matrix')\n",
    " print(confusion_matrix(y_test,pred))\n",
    "  # Print out confusion matrix\n",
    " cmat = confusion_matrix(y_test, pred)\n",
    "#print(cmat)\n",
    " print('TP - True Negative {}'.format(cmat[0,0]))\n",
    " print('FP - False Positive {}'.format(cmat[0,1]))\n",
    " print('FN - False Negative {}'.format(cmat[1,0]))\n",
    " print('TP - True Positive {}'.format(cmat[1,1]))\n",
    "#Accuracy Rate\n",
    " Accuracy = (np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat)))\n",
    " print('Accuracy Rate:',Accuracy)\n",
    " #interval = z * sqrt( (accuracy * (1 - accuracy)) / n)\n",
    "    #confidence interval with 80% \n",
    " confidence_interval = 1.282 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    " print(\"True error (with 80% Confidence Interval): \", confidence_interval)\n",
    " confidence_interval2 = 1.645 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    " print(\"True error (with 90% Confidence Interval): \", confidence_interval2)\n",
    "    # printing pessimistic error of the    \n",
    "    #pessimistic error\n",
    " print('Pessimistic Error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))\n",
    " print('\\n')   \n",
    " \n",
    "print('-> 5-Fold cross-validation accurcay score for the training data for above classifiers')\n",
    "print('test results',results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RIPPER(n_discretize_bins=10, verbosity=0, max_total_conds=None, max_rules=None, random_state=None, dl_allowance=64, k=2, max_rule_conds=None, prune_size=0.33)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# applying ripper algorithm\n",
    "import wittgenstein as lw\n",
    "ripper_clf = lw.RIPPER() # Or irep_clf = lw.IREP() to build a model using IREP\n",
    "ripper_clf.fit( X_train, y_train) # Or pass X and y data to .fit\n",
    "ripper_clf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Ruleset [marital-status=2^education-num=0.8-1.0^capital-gain=0.0-1.0^hours-per-week=0.5-0.7] V [marital-status=2^education-num=0.8-1.0^occupation=2.0-3.0^hours-per-week=0.4-0.5^capital-loss=0.0-0.98] V [marital-status=2^education-num=0.8-1.0^capital-gain=0.0-1.0^hours-per-week=0.4-0.5^occupation=7.0-9.0] V [marital-status=2^education-num=0.6-0.8^occupation=2.0-3.0^hours-per-week=0.4-0.5^capital-loss=0.0-0.98] V [marital-status=2^education-num=0.6-0.8^capital-gain=0.0-1.0^occupation=2.0-3.0^race=4^age=0.34-0.41] V [marital-status=2^education-num=0.6-0.8^capital-gain=0.0-1.0^hours-per-week=0.4-0.5^workclass=2^occupation=7.0-9.0] V [marital-status=2^education-num=0.8-1.0^occupation=2.0-3.0^workclass=2^capital-gain=0.0-1.0] V [marital-status=2^education-num=0.6-0.8^occupation=2.0-3.0^hours-per-week=0.4-0.5^age=0.41-0.49] V [marital-status=2^education-num=0.8-1.0^capital-loss=0.0-0.98] V [marital-status=2^education-num=0.6-0.8^capital-gain=0.0-1.0^occupation=2.0-3.0] V [marital-status=2^education-num=0.6-0.8^capital-gain=0.0-1.0] V [marital-status=2^education-num=0.8-1.0^occupation=2.0-3.0^hours-per-week=0.4-0.5^race=4^age=0.41-0.49] V [marital-status=2^education-num=0.6-0.8^hours-per-week=0.4-0.5^occupation=2.0-3.0^workclass=2^age=0.49-0.63] V [marital-status=2^occupation=2.0-3.0^education-num=0.8-1.0^hours-per-week=0.4-0.5^workclass=2^age=0.23-0.29] V [marital-status=2^education-num=0.8-1.0^occupation=7.0-9.0^hours-per-week=0.38-0.4^workclass=2^race=4^capital-gain=0.0-0.0^age=0.49-0.63] V [marital-status=2^occupation=2.0-3.0^education-num=0.6-0.8^hours-per-week=0.4-0.5] V [marital-status=2^education-num=0.8-1.0^occupation=2.0-3.0^workclass=2^age=0.34-0.41] V [marital-status=2^education-num=0.8-1.0^capital-gain=0.0-1.0^age=0.29-0.34] V [marital-status=2^occupation=7.0-9.0^education-num=0.8-1.0^hours-per-week=0.38-0.4^race=4] V [marital-status=2^education-num=0.6-0.8^capital-loss=0.0-0.98^workclass=2^age=0.23-0.29] V [marital-status=2^education-num=0.6-0.8^occupation=7.0-9.0^workclass=2^age=0.34-0.41^hours-per-week=0.38-0.4] V [marital-status=2^occupation=2.0-3.0^education-num=0.8-1.0^hours-per-week=0.5-0.7] V [marital-status=2^occupation=7.0-9.0^education-num=0.8-1.0^age=0.29-0.34^relationship=0^education=11.0-15.0^hours-per-week=0.4-0.5] V [marital-status=2^education-num=0.6-0.8^capital-loss=0.0-0.98^occupation=9.0-11.0^hours-per-week=0.4-0.5] V [marital-status=2^education-num=0.6-0.8^occupation=2.0-3.0^capital-loss=0.0-0.98] V [marital-status=2^education-num=0.6-0.8^occupation=7.0-9.0] V [marital-status=2^occupation=2.0-3.0^capital-gain=0.0-1.0^education=11.0-15.0^workclass=3] V [marital-status=2^education-num=0.8-1.0^age=0.41-0.49^race=4^workclass=3] V [marital-status=2^education=11.0-15.0^education-num=0.8-1.0^occupation=7.0-9.0^workclass=4^hours-per-week=0.5-0.7] V [marital-status=2^occupation=2.0-3.0^workclass=2^age=0.34-0.41^relationship=5] V [marital-status=2^occupation=2.0-3.0^workclass=2^education-num=0.8-1.0^age=0.29-0.34] V [marital-status=2^education=11.0-15.0^education-num=0.8-1.0] V [marital-status=2^education-num=0.6-0.8^occupation=2.0-3.0] V [marital-status=2^education-num=0.53-0.6^occupation=2.0-3.0^age=0.34-0.41^hours-per-week=0.4-0.5] V [marital-status=2^capital-gain=0.0-1.0^occupation=2.0-3.0^age=0.41-0.49] V [marital-status=2^education-num=0.53-0.6^occupation=2.0-3.0^capital-gain=0.0-1.0^relationship=0^workclass=2] V [marital-status=2^education-num=0.6-0.8^occupation=9.0-11.0^age=0.23-0.29^workclass=2] V [marital-status=2^education-num=0.53-0.6^capital-gain=0.0-1.0^age=0.34-0.41^hours-per-week=0.38-0.4^workclass=2] V [marital-status=2^education-num=0.53-0.6^age=0.41-0.49^workclass=3^occupation=2.0-3.0] V [marital-status=2^education-num=0.6-0.8^occupation=9.0-11.0^workclass=3^hours-per-week=0.4-0.5] V [marital-status=2^education-num=0.6-0.8^occupation=9.0-11.0^capital-loss=0.0-0.98] V [marital-status=2^education-num=0.53-0.6^occupation=2.0-3.0^age=0.49-0.63] V [marital-status=2^education-num=0.53-0.6^capital-gain=0.0-1.0^race=2] V [marital-status=2^education-num=0.6-0.8^age=0.34-0.41^occupation=0.0-0.0^relationship=0^race=4] V [marital-status=2^education-num=0.53-0.6^capital-loss=0.0-0.98^age=0.34-0.41^hours-per-week=0.38-0.4] V [marital-status=2^capital-gain=0.0-1.0^age=0.49-0.63^occupation=0.0-2.0^hours-per-week=0.38-0.4] V [marital-status=2^education-num=0.53-0.6^age=0.41-0.49^workclass=2^occupation=0.0-2.0] V [marital-status=2^capital-gain=0.0-1.0^age=0.41-0.49^education=9.0-11.0] V [marital-status=2^education-num=0.53-0.6^capital-gain=0.0-1.0^age=0.34-0.41] V [marital-status=2^education-num=0.6-0.8^occupation=9.0-11.0] V [marital-status=2^education-num=0.53-0.6^workclass=3^hours-per-week=0.5-0.7^age=0.23-0.29] V [marital-status=2^education-num=0.53-0.6^age=0.49-0.63^capital-loss=0.0-0.98^workclass=2] V [marital-status=2^hours-per-week=0.4-0.5^occupation=7.0-9.0^education-num=0.8-1.0] V [marital-status=2^education-num=0.53-0.6^hours-per-week=0.4-0.5^age=0.29-0.34^occupation=9.0-11.0] V [marital-status=2^education-num=0.53-0.6^age=0.41-0.49^workclass=2] V [marital-status=2^occupation=2.0-3.0^capital-loss=0.0-0.98^education=9.0-11.0] V [marital-status=2^hours-per-week=0.4-0.5^age=0.34-0.41^workclass=2^education=5.0-9.0] V [marital-status=2^capital-gain=0.0-1.0^occupation=2.0-3.0^age=0.49-0.63] V [marital-status=2^education-num=0.6-0.8^age=0.29-0.34^occupation=0.0-0.0] V [marital-status=2^hours-per-week=0.4-0.5^capital-gain=0.0-1.0^age=0.29-0.34] V [marital-status=2^education-num=0.53-0.6^age=0.34-0.41^occupation=9.0-11.0^workclass=2] V [marital-status=2^education-num=0.53-0.6^occupation=2.0-3.0^capital-loss=0.0-0.98] V [marital-status=2^education-num=0.53-0.6^workclass=0^occupation=0.0-0.0] V [marital-status=2^age=0.34-0.41^occupation=0.0-0.0^race=2] V [marital-status=2^education=9.0-11.0^capital-gain=0.0-1.0^age=0.23-0.29^occupation=9.0-11.0] V [marital-status=2^capital-gain=0.0-1.0^age=0.34-0.41^hours-per-week=0.38-0.4]>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ruelset of ripper algo\n",
    "ripper_clf.ruleset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8847386451541607\n",
      "Pessimistic error: 0.20035363023538513\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:',ripper_clf.score(X_test, y_test))\n",
    "print('Pessimistic error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision ripper: 0.0 recall ripper: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#performance of ripper\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = ripper_clf.score(X_test, y_test, precision_score)\n",
    "recall = ripper_clf.score(X_test, y_test, recall_score)\n",
    "print(f'precision ripper: {precision} recall ripper: {recall}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Clustering data1 - titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Family</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass     Sex Embarked  Family\n",
       "0            1         0       3    male        S       2\n",
       "1            2         1       1  female        C       2\n",
       "2            3         1       3  female        S       1\n",
       "3            4         1       1  female        S       2\n",
       "4            5         0       3    male        S       1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing data\n",
    "\n",
    "df_train= pd.read_csv(\"dataClustering/train.csv\")\n",
    "df_test= pd.read_csv(\"dataClustering/test.csv\")\n",
    "\n",
    "\n",
    "\n",
    "#preprocessing\n",
    "df_train = df_train.drop('Name', axis=1,)\n",
    "df_train = df_train.drop('Ticket', axis=1,)\n",
    "df_train = df_train.drop('Fare', axis=1,)\n",
    "df_train = df_train.drop('Cabin', axis=1,)\n",
    "\n",
    "df_train['Family'] = df_train['SibSp'] + df_train['Parch'] + 1\n",
    "\n",
    "df_train = df_train.drop('SibSp', axis=1,)\n",
    "df_train = df_train.drop('Parch', axis=1,)\n",
    "\n",
    "df_train[\"Age\"] = df_train[\"Age\"].fillna(df_train[\"Age\"].median())\n",
    "df_train[\"Embarked\"] = df_train[\"Embarked\"].fillna(\"S\")\n",
    "df_train = df_train.drop('Age', axis=1,)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_train.filter(['Pclass','Sex','Embarked','Family','Adult'], axis=1)\n",
    "\n",
    "X = df1\n",
    "\n",
    "df2 = df_train['Survived']\n",
    "\n",
    "y = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dropping the Embarked and Family column\n",
    "\n",
    "X = X.drop('Embarked', axis=1,)\n",
    "X = X.drop('Family', axis=1,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing and spliting data\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "features_train['Sex'] = lb_make.fit_transform(features_train['Sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "lb_make = LabelEncoder()\n",
    "features_test['Sex'] = lb_make.fit_transform(features_test \n",
    "                                             ['Sex'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Titanic dataset \n",
      "model mean standard deviation\n",
      "     Mean    Standard Deviation\n",
      "KNN: 0.818216 (0.004867)\n",
      "confusion matrix\n",
      "[[152   5]\n",
      " [ 56  55]]\n",
      "TP - True Negative 152\n",
      "FP - False Positive 5\n",
      "FN - False Negative 56\n",
      "TP - True Positive 55\n",
      "Accuracy Rate: 0.7723880597014925\n",
      "True error (with 80% Confidence Interval):  0.02153571514161661\n",
      "True error (with 90% Confidence Interval):  0.02763358144146593\n",
      "Pessimistic error: 0.22761194029850745\n",
      "\n",
      "\n",
      "\n",
      "     Mean    Standard Deviation\n",
      "ID3: 0.810969 (0.009602)\n",
      "confusion matrix\n",
      "[[152   5]\n",
      " [ 56  55]]\n",
      "TP - True Negative 152\n",
      "FP - False Positive 5\n",
      "FN - False Negative 56\n",
      "TP - True Positive 55\n",
      "Accuracy Rate: 0.7723880597014925\n",
      "True error (with 80% Confidence Interval):  0.02153571514161661\n",
      "True error (with 90% Confidence Interval):  0.02763358144146593\n",
      "Pessimistic error: 0.22761194029850745\n",
      "\n",
      "\n",
      "\n",
      "     Mean    Standard Deviation\n",
      "NB: 0.804481 (0.006848)\n",
      "confusion matrix\n",
      "[[134  23]\n",
      " [ 33  78]]\n",
      "TP - True Negative 134\n",
      "FP - False Positive 23\n",
      "FN - False Negative 33\n",
      "TP - True Positive 78\n",
      "Accuracy Rate: 0.7910447761194029\n",
      "True error (with 80% Confidence Interval):  0.020881954819997876\n",
      "True error (with 90% Confidence Interval):  0.026794708017859988\n",
      "Pessimistic error: 0.208955223880597\n",
      "\n",
      "\n",
      "\n",
      "-> 5-Fold cross-validation accurcay score for the training data for above classifiers\n",
      "test results [0.7723880597014925, 0.7723880597014925, 0.7910447761194029]\n"
     ]
    }
   ],
   "source": [
    "# applying 5NN , id3, Naive bayes\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from statistics import mean, stdev\n",
    "# visualization\n",
    "import seaborn as sns \n",
    "\n",
    "# Spot-Check Algorithms\n",
    "models = []\n",
    "\n",
    "models.append(( 'KNN' , KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)))\n",
    "models.append(( 'ID3' , DecisionTreeClassifier(criterion = 'entropy', random_state = 0)))\n",
    "models.append(( 'NB' , GaussianNB()))\n",
    "\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 5\n",
    "num_instances = len(features_train)\n",
    "seed = 7 \n",
    "scoring =  'accuracy'\n",
    "\n",
    "# Test options and evaluation metric\n",
    "num_folds = 5\n",
    "num_instances = len(features_train)\n",
    "seed = 7 \n",
    "scoring =  'accuracy'\n",
    "results = []\n",
    "names = []\n",
    "print(\"For Titanic dataset \")\n",
    "print('model mean standard deviation')\n",
    "for name, model in models:\n",
    " \n",
    " cv_results = StratifiedKFold(n_splits=5, random_state=1)\n",
    " scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv_results, n_jobs=-1)\n",
    " model.fit(features_train,labels_train) \n",
    " \n",
    " pred = model.predict(features_test)\n",
    " \n",
    " results.append(model.score(features_test, labels_test))\n",
    " names.append(name)\n",
    " msg = \"%s: %f (%f)\" % (name,scores.mean(), scores.std())\n",
    " print('     Mean    Standard Deviation')\n",
    " print(msg)\n",
    " print('confusion matrix')\n",
    " print(confusion_matrix(labels_test,pred))\n",
    "  # Print out confusion matrix\n",
    " cmat = confusion_matrix(labels_test, pred)\n",
    "\n",
    " print('TP - True Negative {}'.format(cmat[0,0]))\n",
    " print('FP - False Positive {}'.format(cmat[0,1]))\n",
    " print('FN - False Negative {}'.format(cmat[1,0]))\n",
    " print('TP - True Positive {}'.format(cmat[1,1]))\n",
    "    #accuracy\n",
    " Accuracy = (np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat)))\n",
    " print('Accuracy Rate:',Accuracy)\n",
    " #interval = z * sqrt( (accuracy * (1 - accuracy)) / n)\n",
    "    #confidence interval with 80% \n",
    " confidence_interval = 1.282 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    " print(\"True error (with 80% Confidence Interval): \", confidence_interval)\n",
    " confidence_interval2 = 1.645 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    " print(\"True error (with 90% Confidence Interval): \", confidence_interval2)\n",
    "    #pessimistic error\n",
    " print('Pessimistic error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))\n",
    " print('\\n')\n",
    " print()\n",
    " \n",
    "print('-> 5-Fold cross-validation accurcay score for the training data for above classifiers')\n",
    "print('test results',results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RIPPER(n_discretize_bins=10, verbosity=0, max_total_conds=None, max_rules=None, random_state=None, dl_allowance=64, k=2, max_rule_conds=None, prune_size=0.33)>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# applying ripper algorithm\n",
    "import wittgenstein as lw\n",
    "ripper_clf = lw.RIPPER() # Or irep_clf = lw.IREP() to build a model using IREP\n",
    "ripper_clf.fit( features_train,labels_train) # Or pass X and y data to .fit\n",
    "ripper_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Ruleset [Sex=0^Pclass=2] V [Sex=0^Pclass=1]>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ruelset of ripper algo\n",
    "ripper_clf.ruleset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6324237560192616\n",
      "Pessimistic error: 0.208955223880597\n"
     ]
    }
   ],
   "source": [
    "# ripper test performance\n",
    "\n",
    "print(\"Accuracy:\",ripper_clf.score(features_train,labels_train))\n",
    "print('Pessimistic error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision ripper: 1.0, recall ripper: 0.008658008658008658\n"
     ]
    }
   ],
   "source": [
    "#performance of ripper\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "precision = ripper_clf.score(features_train,labels_train, precision_score)\n",
    "recall = ripper_clf.score(features_train,labels_train, recall_score)\n",
    "print(f'precision ripper: {precision}, recall ripper: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix\n",
      "[[157   0]\n",
      " [109   2]]\n",
      "\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      1.00      0.74       157\n",
      "           1       1.00      0.02      0.04       111\n",
      "\n",
      "    accuracy                           0.59       268\n",
      "   macro avg       0.80      0.51      0.39       268\n",
      "weighted avg       0.76      0.59      0.45       268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred1 = ripper_clf.predict(features_test)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(labels_test,pred1))\n",
    "print('\\n')\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(labels_test,pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP - True Negative 157\n",
      "FP - False Positive 0\n",
      "FN - False Negative 109\n",
      "TP - True Positive 2\n",
      "Accuracy Rate: 0.5932835820895522\n",
      "True error (with 80% Confidence Interval):  0.02523021892538764\n",
      "True error (with 90% Confidence Interval):  0.03237418887071971\n",
      "Pessimistic Error: 0.40671641791044777\n",
      "\n",
      "\n",
      "Pessimistic Error: 0.40671641791044777\n"
     ]
    }
   ],
   "source": [
    "# Print out confusion matrix\n",
    "cmat = confusion_matrix(labels_test, pred1)\n",
    "#print(cmat)\n",
    "print('TP - True Negative {}'.format(cmat[0,0]))\n",
    "print('FP - False Positive {}'.format(cmat[0,1]))\n",
    "print('FN - False Negative {}'.format(cmat[1,0]))\n",
    "print('TP - True Positive {}'.format(cmat[1,1]))\n",
    "Accuracy = (np.divide(np.sum([cmat[0,0],cmat[1,1]]),np.sum(cmat)))\n",
    "print('Accuracy Rate:',Accuracy)\n",
    " #interval = z * sqrt( (accuracy * (1 - accuracy)) / n)\n",
    "    #confidence interval with 80% \n",
    "confidence_interval = 1.282 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    "print(\"True error (with 80% Confidence Interval): \", confidence_interval)\n",
    "confidence_interval2 = 1.645 * sqrt( Accuracy * (1 - Accuracy)/num_instances)\n",
    "print(\"True error (with 90% Confidence Interval): \", confidence_interval2)\n",
    "print('Pessimistic Error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))\n",
    "print('\\n')\n",
    "# Pessimistic error\n",
    "print('Pessimistic Error: {}'.format(np.divide(np.sum([cmat[0,1],cmat[1,0]]),np.sum(cmat))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
